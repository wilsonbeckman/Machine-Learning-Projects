{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acc10138",
   "metadata": {},
   "source": [
    "Welcome to my example of a Generative Pretrained Transformer (GPT)!\n",
    "\n",
    "This code was inspired by Andrej Karpathy's lecture on Nano GPT with Shakespeare.(https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "In this example I built a similar model using Beatles lyrics, so the model outputs new lyrics using the text dataset I found online that contains most of the Beatles Discography.\n",
    "\n",
    "This particular model omits the encoding block and the cross attention block from the original transformer model. The reason for this is we're generating text from scratch and it's not conditioned on any data. If we added an encoder model, it would just be a machine translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf0329",
   "metadata": {},
   "source": [
    "The Hyperparameters below describe some of the variables we'll be implementing in our model.\n",
    "\n",
    "Batch Size:\n",
    "Block Size:\n",
    "Max Iteration: \n",
    "Eval Interval: \n",
    "Learning Rate:\n",
    "Eval Iterations:\n",
    "Number of Embeddings:\n",
    "Number of Heads:\n",
    "Number of Layers:\n",
    "Dropout:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8b1bc144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.749529 M parameters\n",
      "step 0: train loss 4.6982, val loss 4.6954\n",
      "step 100: train loss 2.4626, val loss 2.4330\n",
      "step 200: train loss 2.3799, val loss 2.3589\n",
      "step 300: train loss 2.3334, val loss 2.3162\n",
      "step 400: train loss 2.2896, val loss 2.2795\n",
      "step 500: train loss 2.2202, val loss 2.2225\n",
      "step 600: train loss 2.0929, val loss 2.1023\n",
      "step 700: train loss 1.9768, val loss 2.0013\n",
      "step 800: train loss 1.8835, val loss 1.9139\n",
      "step 900: train loss 1.7887, val loss 1.8428\n",
      "step 1000: train loss 1.7004, val loss 1.7857\n",
      "step 1100: train loss 1.6338, val loss 1.7338\n",
      "step 1200: train loss 1.5740, val loss 1.7036\n",
      "step 1300: train loss 1.5132, val loss 1.6657\n",
      "step 1400: train loss 1.4510, val loss 1.6532\n",
      "step 1500: train loss 1.4056, val loss 1.6403\n",
      "step 1600: train loss 1.3620, val loss 1.6225\n",
      "step 1700: train loss 1.3183, val loss 1.6037\n",
      "step 1800: train loss 1.2732, val loss 1.5920\n",
      "step 1900: train loss 1.2243, val loss 1.5870\n",
      "step 2000: train loss 1.1858, val loss 1.5702\n",
      "step 2100: train loss 1.1490, val loss 1.5684\n",
      "step 2200: train loss 1.1052, val loss 1.5778\n",
      "step 2300: train loss 1.0718, val loss 1.5821\n",
      "step 2400: train loss 1.0323, val loss 1.5772\n",
      "step 2499: train loss 0.9942, val loss 1.5714\n",
      "\n",
      "\"God I'co much.\"\n",
      "\n",
      "\"You so, don't so here,\"\n",
      "\"All, I need will to mud,\"\n",
      "\"What you say were you only to you,\"\n",
      "Thou'll coming mon.\n",
      "\n",
      "\"Take you feel it hangookin' a having imme,\"\n",
      "Living oven walk of you culll back te.\n",
      "\n",
      "\"Remember in all, roll man,\"\n",
      "And some for throuning ms need have ight\n",
      "Don't fineorging of the shilf\"\n",
      "\"Tuning thinks this mirde wind the moris frieng, pille The mingirl sing\"\n",
      "We Su cup tmy onight.\n",
      "When your cryind gren\n",
      "Alothe a ntight for rom me\n",
      "but the to licks with his and\n",
      "'caufer the terend ruest like that but a mond'at crose.\n",
      "(cause Fr com you con't seeave--yeah one on treally me)\n",
      "My like these dome\n",
      "A the peopiness ow in they (she's dappy)\n",
      "When do they lied yet ground)\n",
      "\"Everybody to get my (time day)\n",
      "Yes a long ot someop seedie.)\n",
      "\n",
      "And yes ithin old you sleee your right\n",
      "And gaver he'll oget be the me?\n",
      "\"Can but side he Judes ince, 'cause beat me\"\n",
      "\"Tel me heart sagt, thave one\"\n",
      "\"Bream think can, I mearss dam,\"\n",
      "minu'rus he begir mery\n",
      "\n",
      "\"A surre he don't mrine,\"\n",
      "\"Sh he gies a niver oght.\n",
      "\n",
      "\"Nuarke you tonight's a can driven, give myb.Sand ning ight time.\"\n",
      "\"Can, you're seen suchildon\"\n",
      "bluest nights anight.\n",
      "Bill Moall's she long alonly fill\n",
      "Al And recock from than I do)\n",
      "\"All my litge a light she dens noter nightÂ \"\n",
      "Till be sad\n",
      "Will be it wrigh you\n",
      "\n",
      "We've got a dock beling my work to\n",
      "Something or time I'm styaways stop\n",
      "\n",
      "Howe carry bry cruping around doesn'\n",
      "\"I someone, My diss, I get iss sicheas\"\n",
      "And I can prememst words\n",
      "I'll red matchan in the beathrings\n",
      "\n",
      "But in the card of lovef with momentil life\n",
      "In him stend in acause a call\n",
      "I tast aill is rockear holl you\n",
      "I mearted anybow I'm going byou the uss long down to seectafor to babe)\n",
      "\n",
      "I have been ocking throos, so.\"\n",
      "All you who're and the scriting and on the carn cappengir of theirs\n",
      "All trock the birthdant icon beath walk hort ins the said to splead.\n",
      "\n",
      "It's enothing of the sking of but rallow\n",
      "Trighting my if you\n",
      "\n",
      "It's beeen a lonk acrid thre's man\n",
      "Ache I tim yampare.\n",
      "And the wagy of justying not.\n",
      "The can world that's l\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import requests\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 2500\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 192\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "#Import the text\n",
    "url = \"https://storage.googleapis.com/kagglesdsdata/datasets/271190/563561/beatles.txt?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230411%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230411T192247Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=509dcf33d307dbf948b0b3d2a900cecbaa00d9f70e0ad00e575a4d8d095d58813ebd0d72599022cba1aec02ff7dacc2c065860302a1ba60d85e9434133777baa6f8d0d08fa445f4611a830fa21507f4e1c3a78e9d2cb01848952202358e2fd834a8553d440e9125542b62ad7a747f3936f95e68a4f4c8a0c4d1e7d7cd16ba2a131c34335dc04c18678a37fa067cd200f733bca31f586ea4931ec3717d37985a32e0cfeb3f5da00deca4f10e5ac6dd2473af36a1e8062c320ffabc1e04daf9e580434dbc3ab2a913988e2bd7897e258f1da025d3a07461a3380ffd2f8c1670bb22c0cb342dc1fce70fc8cf189145eb581d13d42069acd7f211f263533b5762829\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e939b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
